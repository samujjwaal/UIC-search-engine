{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler Web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T04:29:59.064233Z",
     "start_time": "2020-04-30T04:29:54.037766Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dependency libraries\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T06:52:29.212246Z",
     "start_time": "2020-04-30T06:52:28.141599Z"
    }
   },
   "outputs": [],
   "source": [
    "domain = \"uic.edu\"\n",
    "start_url = \"https://www.cs.uic.edu/\"\n",
    "\n",
    "pages_folder = \"./FetchedPages/\"\n",
    "\n",
    "pickle_folder = \"./PickelFiles/\"\n",
    "os.makedirs(pickle_folder, exist_ok=True)\n",
    "\n",
    "# file extensions to ignore while crawling pages\n",
    "ignore_ext = [\n",
    "    '.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx', '.css', '.js',\n",
    "    '.aspx', '.png', '.jpg', '.jpeg', '.JPG', '.gif', '.svg', '.ico', '.mp4',\n",
    "    '.avi', '.tar', '.gz', '.tgz', '.zip'\n",
    "]\n",
    "\n",
    "crawl_limit = 3000\n",
    "\n",
    "# to make sure error log file is initially empty\n",
    "error_file = \"error_log.txt\"\n",
    "os.makedirs(os.path.dirname(error_file), exist_ok=True)\n",
    "f = open(error_file, \"r+\")\n",
    "f.truncate()\n",
    "f.close()\n",
    "\n",
    "# queue to perform BFS web traversal\n",
    "url_q = deque()\n",
    "url_q.append(start_url)\n",
    "\n",
    "# list to keep track of traversed URLs\n",
    "urls_crawled = []\n",
    "urls_crawled.append(start_url)\n",
    "\n",
    "# dict to track pages fetched and stored in folder\n",
    "pages_crawled = {}\n",
    "page_no = 0\n",
    "\n",
    "while url_q:\n",
    "    \n",
    "    try:\n",
    "        url = url_q.popleft()               # fetch the first URL from the queue\n",
    "        rqst = requests.get(url)            # get html code of web page\n",
    "\n",
    "        if (rqst.status_code == 200):\n",
    "            pages_crawled[page_no] = url    \n",
    "\n",
    "            output_file = pages_folder + str(page_no)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)     # create file to store html code\n",
    "\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(rqst.text)\n",
    "            file.close()\n",
    "\n",
    "            soup = BeautifulSoup(rqst.text, 'lxml')\n",
    "            tags_extracted = soup.find_all('a')                 # extract all 'a' tags from page\n",
    "\n",
    "            for tag in tags_extracted:\n",
    "                \n",
    "                l = tag.get('href')\n",
    "\n",
    "                if l is not None and l.startswith(\"http\"):\n",
    "\n",
    "                    if not any(ext in l for ext in ignore_ext):\n",
    "\n",
    "                        if l not in urls_crawled and domain in l:\n",
    "#                             print(l)\n",
    "                            url_q.append(l)                 # valid URL to append to the queue\n",
    "                            urls_crawled.append(l)\n",
    "\n",
    "            if (len(pages_crawled) > crawl_limit):\n",
    "                break                                        # stop crawling when reached limit\n",
    "\n",
    "            page_no += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(error_file, \"a+\") as log:                  # add error message to error log\n",
    "            log.write(f\"Could not connect to {url}\")\n",
    "            log.write(f\"\\nError occured: {e}\\n\\n\")\n",
    "        file.close()\n",
    "\n",
    "        print(\"Could not connect to \", url)\n",
    "        print(\"Error occured: \", e, \" \\n\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "with open(pickle_folder + 'pages_crawled.pickle', 'wb') as f:\n",
    "    pickle.dump(pages_crawled,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T05:58:36.312093Z",
     "start_time": "2020-04-28T05:58:36.304085Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(pages_crawled)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T07:26:07.984013Z",
     "start_time": "2020-04-28T07:26:07.968007Z"
    }
   },
   "outputs": [],
   "source": [
    "# pages_crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T07:25:49.678885Z",
     "start_time": "2020-04-28T07:25:49.658884Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('pages.pickle', 'rb') as f:\n",
    "    pages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T07:26:00.673561Z",
     "start_time": "2020-04-28T07:26:00.655571Z"
    }
   },
   "outputs": [],
   "source": [
    "# pages"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
