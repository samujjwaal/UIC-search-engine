{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:14:14.206905Z",
     "start_time": "2020-04-30T07:14:14.186937Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dependency libraries\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from collections import deque\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:16:38.966585Z",
     "start_time": "2020-04-30T07:16:38.956996Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting english stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initializing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "# Initializing regex to remove words with one or two characters length\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "# folder to store pickel files\n",
    "pickle_folder = \"./PickelFiles/\"\n",
    "os.makedirs(pickle_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:16:39.394846Z",
     "start_time": "2020-04-30T07:16:39.381840Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_folder = \"./FetchedPages/\"\n",
    "filenames = os.listdir(pages_folder)\n",
    "\n",
    "# list to store filenames of all stored crawled webpages\n",
    "files = []\n",
    "\n",
    "for name in filenames:\n",
    "    files.append(name)\n",
    "\n",
    "# len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:16:39.762937Z",
     "start_time": "2020-04-30T07:16:39.750733Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# for file in files[:1]:\n",
    "#     web_page = open(pages_folder + file, \"r\", encoding=\"utf-8\")\n",
    "#     code = web_page.read()\n",
    "# #     print(code)\n",
    "#     soup = BeautifulSoup(code, \"html.parser\")\n",
    "#     [s.extract() for s in soup(['style', 'script', '[document]', 'head'])]\n",
    "#     visible_text = soup.getText()\n",
    "#     print(visible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:16:40.269608Z",
     "start_time": "2020-04-30T07:16:40.257747Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to filter tags that are visible on webpage i.e. excluding style, script, meta, etc. tags\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'meta', '[document]']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):                     # check if element is html comment\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)):              #  to eliminate remaining extra white spaces and new lines\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:16:40.665259Z",
     "start_time": "2020-04-30T07:16:40.652256Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to extract only the visible text from the html code of each webpage\n",
    "\n",
    "def get_text_from_code(page):\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text_in_page = soup.find_all(text=True)                # return all text in page\n",
    "    visible_text = filter(tag_visible, text_in_page)       # return only visible text\n",
    "    return \" \".join(term.strip() for term in visible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:20:29.458341Z",
     "start_time": "2020-04-30T07:16:47.533919Z"
    }
   },
   "outputs": [],
   "source": [
    "# dict to create inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# dict to store tokens in each web page\n",
    "webpage_tokens = {}\n",
    "\n",
    "for file in files:\n",
    "    web_page = open(pages_folder + file, \"r\", encoding=\"utf-8\")\n",
    "    code = web_page.read()\n",
    "#     print(code)\n",
    "    text = get_text_from_code(code)                     # get all text actually visible on web page\n",
    "#     print(text,\"\\n\")\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z]+', ' ', text)                 # remove all punctuations and digits\n",
    "    \n",
    "    tokens = text.split()\n",
    "#     print(tokens, \"\\n\")\n",
    "    \n",
    "#     # removing stop words from the tokens\n",
    "#     clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "#     # stemming the tokens\n",
    "#     stem_tokens = [st.stem(word) for word in clean_tokens]\n",
    "\n",
    "#     # checking for stopwords again\n",
    "#     clean_stem_tokens = [word for word in stem_tokens if word not in stop_words]\n",
    "\n",
    "#     # converting list of tokens to string\n",
    "#     clean_stem_tokens = ' '.join(map(str,  clean_stem_tokens))\n",
    "\n",
    "#     # removing tokens with one or two characters length\n",
    "#     clean_stem_tokens = shortword.sub('', clean_stem_tokens)\n",
    "#     print(clean_stem_tokens, \"\\n\")\n",
    "    \n",
    "    # removing stop words and stemming each token while only accepting stemmed tokens with length greater than 2 \n",
    "    clean_stem_tokens = [\n",
    "        st.stem(token) for token in tokens \n",
    "        if (token not in stop_words and st.stem(token) not in stop_words) and len(st.stem(token))>2\n",
    "    ]\n",
    "#     print(clean_stem_tokens, \"\\n\")\n",
    "    \n",
    "    webpage_tokens[file] = clean_stem_tokens                        # add tokens in web page to dict \n",
    "    \n",
    "    for token in clean_stem_tokens:\n",
    "        \n",
    "        freq = inverted_index.setdefault(token,{}).get(file,0)      # get frequency of token and set to 0 if token not in dict\n",
    "            \n",
    "        inverted_index.setdefault(token,{})[file] = freq  + 1       # add 1 to frequency of token in current webpage\n",
    "\n",
    "#         inverted_index.setdefault(token,{})[file] = inverted_index.setdefault(token,{})\n",
    "#         x = inverted_index.setdefault(token,{})[file]\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:44:10.669457Z",
     "start_time": "2020-04-30T07:44:10.253682Z"
    }
   },
   "outputs": [],
   "source": [
    "# pickling inverted index and tokens\n",
    "\n",
    "with open(pickle_folder + 'inverted_index.pickle', 'wb') as f:\n",
    "    pickle.dump(inverted_index,f)\n",
    "    \n",
    "with open(pickle_folder + 'webpages_tokens.pickle', 'wb') as f:\n",
    "    pickle.dump(webpage_tokens,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T07:46:17.934283Z",
     "start_time": "2020-04-30T07:46:17.920181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24598"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_index)\n",
    "# inverted_index"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ds] *",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
